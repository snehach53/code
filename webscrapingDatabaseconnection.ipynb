{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSAe7fjDqvWC9l4WBNBlUk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snehach53/code/blob/main/webscrapingDatabaseconnection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FevjNNXKoB4D",
        "outputId": "c132b680-9c4a-4279-cfa4-6b54926ddcae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Use headers to mimic a browser visit (optional but recommended)\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "url = \"https://www.metacareers.com/jobs?teams[0]=Artificial%20Intelligence&roles[0]=Full%20time%20employment&offices[0]=Menlo%20Park%2C%20CA&offices[1]=Seattle%2C%20WA&offices[2]=New%20York%2C%20NY\"\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    print(\"Page fetched successfully!\")\n",
        "else:\n",
        "    print(f\"Failed to fetch page. Status code: {response.status_code}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLCuQSKG5azB",
        "outputId": "68a0cc33-9ea5-444f-911f-1fe5e3cbedf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch page. Status code: 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1mkV3py66aZh",
        "outputId": "f18554e3-7688-4d34-a217-6a688eebe881"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.91.81)] [Co\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r                                                                               \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,296 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,557 kB]\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,776 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,037 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,750 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,986 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,250 kB]\n",
            "Fetched 23.0 MB in 3s (8,598 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 libudev1 snapd squashfs-tools\n",
            "  systemd-hwe-hwdb udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 snapd\n",
            "  squashfs-tools systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 8 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 30.3 MB of archives.\n",
            "After this operation, 123 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.16 [76.7 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.16 [1,557 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.67.1+22.04 [27.8 MB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Fetched 30.3 MB in 2s (19.8 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 126111 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.16_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.16) over (249.11-0ubuntu3.12) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.16) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 126311 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.16_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.16) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.67.1+22.04_amd64.deb ...\n",
            "Unpacking snapd (2.67.1+22.04) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.16) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.67.1+22.04) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 126540 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.16) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.33.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: urllib3~=2.4.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (2.4.0)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.4.26 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.4.26)\n",
            "Collecting typing_extensions~=4.13.2 (from selenium)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.33.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, typing_extensions, outcome, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.14.0\n",
            "    Uninstalling typing_extensions-4.14.0:\n",
            "      Successfully uninstalled typing_extensions-4.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed outcome-1.3.0.post0 selenium-4.33.0 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.13.2 wsproto-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing_extensions"
                ]
              },
              "id": "3949e773323b46acb5ccf6e3c66febfa"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "driver.get(\"https://www.metacareers.com/jobs\")\n",
        "html = driver.page_source\n",
        "driver.quit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "VELpkVJ76v2-",
        "outputId": "0f948ff6-e982-44f3-9661-4268dd659f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'driver' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-719480240>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.metacareers.com/jobs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'driver' is not defined"
          ]
        }
      ]
    },
    {
      "source": [
        "# Import the webdriver module\n",
        "from selenium import webdriver\n",
        "\n",
        "# Install required packages and configure chromedriver\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n",
        "\n",
        "# Create a Chrome webdriver instance\n",
        "# Make sure the chromedriver is in your system's PATH or specify the executable_path\n",
        "driver = webdriver.Chrome()\n",
        "\n",
        "# Now you can use the driver object\n",
        "driver.get(\"https://www.metacareers.com/jobs\")\n",
        "html = driver.page_source\n",
        "driver.quit()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3HYUZiUJ7CfZ",
        "outputId": "9228ef2c-cd26-4c6a-87b1-6956f439db5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.81)] [Waiting for headers] [C\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Connected\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (1:85.0.4183.83-0ubuntu2.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.33.0)\n",
            "Requirement already satisfied: urllib3~=2.4.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: trio~=0.30.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.12.2 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2025.4.26 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.4.26)\n",
            "Requirement already satisfied: typing_extensions~=4.13.2 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.13.2)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SessionNotCreatedException",
          "evalue": "Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir\nStacktrace:\n#0 0x5a9defc8dc9a <unknown>\n#1 0x5a9def7336e0 <unknown>\n#2 0x5a9def76e0b2 <unknown>\n#3 0x5a9def76924f <unknown>\n#4 0x5a9def7b98b6 <unknown>\n#5 0x5a9def7b8f76 <unknown>\n#6 0x5a9def7aac03 <unknown>\n#7 0x5a9def77747b <unknown>\n#8 0x5a9def7780e1 <unknown>\n#9 0x5a9defc5244b <unknown>\n#10 0x5a9defc5637f <unknown>\n#11 0x5a9defc39f89 <unknown>\n#12 0x5a9defc56f18 <unknown>\n#13 0x5a9defc1e6df <unknown>\n#14 0x5a9defc7b308 <unknown>\n#15 0x5a9defc7b4e6 <unknown>\n#16 0x5a9defc8cb76 <unknown>\n#17 0x7da4d13a7ac3 <unknown>\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSessionNotCreatedException\u001b[0m                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-1907808324>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Create a Chrome webdriver instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Make sure the chromedriver is in your system's PATH or specify the executable_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Now you can use the driver object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/selenium/webdriver/chrome/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moptions\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mbrowser_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDesiredCapabilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHROME\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"browserName\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mvendor_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"goog\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/selenium/webdriver/chromium/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, command_executor, keep_alive, file_detector, options, locator_converter, web_element_cls, client_config)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_authenticator_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fedcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFedCM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mstart_session\u001b[0;34m(self, capabilities)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0mcaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_caps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapabilities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEW_SESSION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sessionId\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"capabilities\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"alert\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mSessionNotCreatedException\u001b[0m: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir\nStacktrace:\n#0 0x5a9defc8dc9a <unknown>\n#1 0x5a9def7336e0 <unknown>\n#2 0x5a9def76e0b2 <unknown>\n#3 0x5a9def76924f <unknown>\n#4 0x5a9def7b98b6 <unknown>\n#5 0x5a9def7b8f76 <unknown>\n#6 0x5a9def7aac03 <unknown>\n#7 0x5a9def77747b <unknown>\n#8 0x5a9def7780e1 <unknown>\n#9 0x5a9defc5244b <unknown>\n#10 0x5a9defc5637f <unknown>\n#11 0x5a9defc39f89 <unknown>\n#12 0x5a9defc56f18 <unknown>\n#13 0x5a9defc1e6df <unknown>\n#14 0x5a9defc7b308 <unknown>\n#15 0x5a9defc7b4e6 <unknown>\n#16 0x5a9defc8cb76 <unknown>\n#17 0x7da4d13a7ac3 <unknown>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "# Set up Chrome options for headless browsing in Colab\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "chrome_options.add_argument(\"--user-data-dir=/tmp/chrome-user-data\")  # Temporary unique path\n",
        "\n",
        "# Initialize the WebDriver\n",
        "driver = webdriver.Chrome(options=chrome_options)\n"
      ],
      "metadata": {
        "id": "GImzU1Fc7f9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Navigate to the Meta Careers page\n",
        "driver.get(\"https://www.metacareers.com/jobs\")\n",
        "\n",
        "# Get page source\n",
        "html = driver.page_source\n",
        "\n",
        "# Close the browser\n",
        "driver.quit()\n",
        "\n",
        "# Parse with BeautifulSoup\n",
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "# Print part of the HTML to inspect\n",
        "print(soup.prettify()[:1000])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uP8J8qTW7ii_",
        "outputId": "caffb632-ed9e-4449-88fb-f23662920a71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<html class=\"\" id=\"facebook\" lang=\"en\">\n",
            " <head>\n",
            "  <meta charset=\"utf-8\"/>\n",
            "  <meta content=\"origin-when-crossorigin\" id=\"meta_referrer\" name=\"referrer\"/>\n",
            "  <script async=\"\" src=\"https://connect.facebook.net/signals/config/720048218061799?v=2.9.208&amp;r=stable&amp;domain=www.metacareers.com&amp;hme=c7027faad2bd527f2b384e7a6d6c55127377ecc46dce76c1ebbdd02e9451da4e&amp;ex_m=81%2C138%2C121%2C15%2C114%2C56%2C37%2C115%2C62%2C55%2C126%2C70%2C10%2C80%2C23%2C109%2C100%2C60%2C63%2C108%2C125%2C6%2C2%2C3%2C5%2C88%2C4%2C71%2C79%2C128%2C129%2C200%2C150%2C50%2C205%2C202%2C203%2C42%2C163%2C22%2C59%2C209%2C208%2C152%2C25%2C49%2C7%2C52%2C75%2C76%2C77%2C82%2C104%2C24%2C21%2C107%2C103%2C102%2C122%2C61%2C124%2C47%2C123%2C38%2C105%2C48%2C97%2C34%2C190%2C192%2C160%2C18%2C19%2C20%2C12%2C13%2C33%2C30%2C31%2C66%2C72%2C74%2C86%2C113%2C116%2C35%2C87%2C16%2C14%2C91%2C57%2C28%2C118%2C117%2C119%2C110%2C17%2C27%2C46%2C85%2C26%2C173%2C146%2C84%2C1%2C106%2C65%2C95%2C41%2C36%2C93%2C94%2C99%2C45%2C11%2C101%2C92%2C53%2C40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "options = Options()\n",
        "options.add_argument(\"--headless\")\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "options.add_argument(\"--disable-dev-shm-usage\")\n",
        "options.add_argument(\"--user-data-dir=/tmp/chrome-data\")\n",
        "\n",
        "driver = webdriver.Chrome(options=options)\n",
        "driver.get(\"https://www.metacareers.com/jobs\")\n",
        "\n",
        "# Scroll down to load listings (optional)\n",
        "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "\n",
        "html = driver.page_source\n",
        "driver.quit()\n",
        "\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "print(soup.prettify()[:2000])  # Inspect the first portion\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MUYRaEF-E2y",
        "outputId": "4d24b88a-28ae-4691-8446-2811294cecb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<html class=\"\" id=\"facebook\" lang=\"en\">\n",
            " <head>\n",
            "  <meta charset=\"utf-8\"/>\n",
            "  <meta content=\"origin-when-crossorigin\" id=\"meta_referrer\" name=\"referrer\"/>\n",
            "  <script async=\"\" src=\"https://connect.facebook.net/signals/config/720048218061799?v=2.9.208&amp;r=stable&amp;domain=www.metacareers.com&amp;hme=c7027faad2bd527f2b384e7a6d6c55127377ecc46dce76c1ebbdd02e9451da4e&amp;ex_m=81%2C138%2C121%2C15%2C114%2C56%2C37%2C115%2C62%2C55%2C126%2C70%2C10%2C80%2C23%2C109%2C100%2C60%2C63%2C108%2C125%2C6%2C2%2C3%2C5%2C88%2C4%2C71%2C79%2C128%2C129%2C200%2C150%2C50%2C205%2C202%2C203%2C42%2C163%2C22%2C59%2C209%2C208%2C152%2C25%2C49%2C7%2C52%2C75%2C76%2C77%2C82%2C104%2C24%2C21%2C107%2C103%2C102%2C122%2C61%2C124%2C47%2C123%2C38%2C105%2C48%2C97%2C34%2C190%2C192%2C160%2C18%2C19%2C20%2C12%2C13%2C33%2C30%2C31%2C66%2C72%2C74%2C86%2C113%2C116%2C35%2C87%2C16%2C14%2C91%2C57%2C28%2C118%2C117%2C119%2C110%2C17%2C27%2C46%2C85%2C26%2C173%2C146%2C84%2C1%2C106%2C65%2C95%2C41%2C36%2C93%2C94%2C99%2C45%2C11%2C101%2C92%2C53%2C40%2C43%2C0%2C78%2C127%2C98%2C9%2C96%2C249%2C188%2C136%2C176%2C169%2C8%2C44%2C73%2C51%2C120%2C54%2C90%2C69%2C68%2C39%2C111%2C67%2C64%2C58%2C89%2C83%2C32%2C112%2C29%2C130\">\n",
            "  </script>\n",
            "  <script async=\"\" src=\"https://connect.facebook.net/en_US/fbevents.js\">\n",
            "  </script>\n",
            "  <script async=\"\" data-testid=\"google-analytics-4-script\" src=\"https://www.googletagmanager.com/gtag/js?id=G-M13JT8ZZ31\">\n",
            "  </script>\n",
            "  <script nonce=\"\">\n",
            "   function envFlush(a){function b(b){for(var c in a)b[c]=a[c]}window.requireLazy?window.requireLazy([\"Env\"],b):(window.Env=window.Env||{},b(window.Env))}envFlush({\"useTrustedTypes\":true,\"isTrustedTypesReportOnly\":true,\"enableDefaultTrustedTypesPolicy\":true,\"defaultTrustedTypesPolicyName\":\"security_infra_logging_FOR_ROLLOUT_ONLY_DO_NOT_USE\",\"ajaxpipe_token\":\"AXhApnfsMFf9Cc_RwBw\",\"stack_trace_limit\":30,\"timesliceBufferSize\":5000,\"show_invariant_decoder\":false,\"compat_iframe_token\":\"AUU7Up0VwEOm2T9Gjney0QPxH_U\",\"isCQuick\":false,\"brsid\":\"7516553564464732066\",\"promise_include_\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all divs and filter by visible job titles\n",
        "job_divs = soup.find_all(\"div\")\n",
        "\n",
        "for div in job_divs:\n",
        "    if \"Software Engineer\" in div.text or \"Engineer\" in div.text:\n",
        "        print(div.text.strip())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyrcDgPT_D2T",
        "outputId": "70913e82-a8a8-4301-e421-c5877e95c69d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JobsTeamsTechnology teamsArtificial IntelligenceCreativeInfrastructureMetaverse and WearablesProduct and Program ManagementResearch and DataSecuritySoftware EngineeringBusiness teamsBusiness OperationsPartnershipsSales and MarketingCareers ProgramsResearchStudents and gradsRotational Product ManagementWorking at MetaAccessibility and EngagementBenefitsCultureHiring processBlogLog in/Create profileSearch Meta CareersHide filtersCareersFollow usJob searchCareer programsResearchStudents and gradsRotational Product ManagementTeamsBusiness teamsTechnology teamsWorking at MetaAccessibility and EngagementBenefitsCultureHiring processMy accountCareer profileAccount settingsMessagesMeta Careers blogAbout usAbout MetaMedia galleryBrand resourcesFor investorsLooking for contractor roles?Equal Employment OpportunityMeta is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, reproductive health decisions, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics. You may view our Equal Employment Opportunity notice here.Meta is committed to providing reasonable support (called accommodations) in our recruiting processes for candidates with disabilities, long term conditions, mental health conditions or sincerely held religious beliefs, or who are neurodivergent or require pregnancy-related support. If you need assistance or an accommodation due to a disability, fill out the  Accommodations request form .© 2025 MetaCommunity StandardsData PolicyTermsCookie Policy\n",
            "JobsTeamsTechnology teamsArtificial IntelligenceCreativeInfrastructureMetaverse and WearablesProduct and Program ManagementResearch and DataSecuritySoftware EngineeringBusiness teamsBusiness OperationsPartnershipsSales and MarketingCareers ProgramsResearchStudents and gradsRotational Product ManagementWorking at MetaAccessibility and EngagementBenefitsCultureHiring processBlogLog in/Create profileSearch Meta CareersHide filtersCareersFollow usJob searchCareer programsResearchStudents and gradsRotational Product ManagementTeamsBusiness teamsTechnology teamsWorking at MetaAccessibility and EngagementBenefitsCultureHiring processMy accountCareer profileAccount settingsMessagesMeta Careers blogAbout usAbout MetaMedia galleryBrand resourcesFor investorsLooking for contractor roles?Equal Employment OpportunityMeta is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, reproductive health decisions, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics. You may view our Equal Employment Opportunity notice here.Meta is committed to providing reasonable support (called accommodations) in our recruiting processes for candidates with disabilities, long term conditions, mental health conditions or sincerely held religious beliefs, or who are neurodivergent or require pregnancy-related support. If you need assistance or an accommodation due to a disability, fill out the  Accommodations request form .© 2025 MetaCommunity StandardsData PolicyTermsCookie Policy\n",
            "JobsTeamsTechnology teamsArtificial IntelligenceCreativeInfrastructureMetaverse and WearablesProduct and Program ManagementResearch and DataSecuritySoftware EngineeringBusiness teamsBusiness OperationsPartnershipsSales and MarketingCareers ProgramsResearchStudents and gradsRotational Product ManagementWorking at MetaAccessibility and EngagementBenefitsCultureHiring processBlogLog in/Create profile\n",
            "JobsTeamsTechnology teamsArtificial IntelligenceCreativeInfrastructureMetaverse and WearablesProduct and Program ManagementResearch and DataSecuritySoftware EngineeringBusiness teamsBusiness OperationsPartnershipsSales and MarketingCareers ProgramsResearchStudents and gradsRotational Product ManagementWorking at MetaAccessibility and EngagementBenefitsCultureHiring processBlogLog in/Create profile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "job_titles = []\n",
        "\n",
        "for div in soup.find_all(\"div\"):\n",
        "    if div.text and (\"Engineer\" in div.text or \"Data\" in div.text or \"Intern\" in div.text):\n",
        "        job_titles.append(div.text.strip())\n",
        "\n",
        "# Optional: remove duplicates\n",
        "job_titles = list(set(job_titles))\n",
        "\n",
        "# Print a few\n",
        "for title in job_titles[:10]:\n",
        "    print(\"Possible Job Title:\", title)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjqd7Iwm_RbF",
        "outputId": "510395be-5d3f-496e-ab84-aa79cdcbe36a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Possible Job Title: © 2025 MetaCommunity StandardsData PolicyTermsCookie Policy\n",
            "Possible Job Title: CareersFollow usJob searchCareer programsResearchStudents and gradsRotational Product ManagementTeamsBusiness teamsTechnology teamsWorking at MetaAccessibility and EngagementBenefitsCultureHiring processMy accountCareer profileAccount settingsMessagesMeta Careers blogAbout usAbout MetaMedia galleryBrand resourcesFor investorsLooking for contractor roles?Equal Employment OpportunityMeta is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, reproductive health decisions, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics. You may view our Equal Employment Opportunity notice here.Meta is committed to providing reasonable support (called accommodations) in our recruiting processes for candidates with disabilities, long term conditions, mental health conditions or sincerely held religious beliefs, or who are neurodivergent or require pregnancy-related support. If you need assistance or an accommodation due to a disability, fill out the  Accommodations request form .© 2025 MetaCommunity StandardsData PolicyTermsCookie Policy\n",
            "Possible Job Title: JobsTeamsTechnology teamsArtificial IntelligenceCreativeInfrastructureMetaverse and WearablesProduct and Program ManagementResearch and DataSecuritySoftware EngineeringBusiness teamsBusiness OperationsPartnershipsSales and MarketingCareers ProgramsResearchStudents and gradsRotational Product ManagementWorking at MetaAccessibility and EngagementBenefitsCultureHiring processBlogLog in/Create profile\n",
            "Possible Job Title: JobsTeamsTechnology teamsArtificial IntelligenceCreativeInfrastructureMetaverse and WearablesProduct and Program ManagementResearch and DataSecuritySoftware EngineeringBusiness teamsBusiness OperationsPartnershipsSales and MarketingCareers ProgramsResearchStudents and gradsRotational Product ManagementWorking at MetaAccessibility and EngagementBenefitsCultureHiring processBlogLog in/Create profileSearch Meta CareersHide filtersCareersFollow usJob searchCareer programsResearchStudents and gradsRotational Product ManagementTeamsBusiness teamsTechnology teamsWorking at MetaAccessibility and EngagementBenefitsCultureHiring processMy accountCareer profileAccount settingsMessagesMeta Careers blogAbout usAbout MetaMedia galleryBrand resourcesFor investorsLooking for contractor roles?Equal Employment OpportunityMeta is proud to be an Equal Employment Opportunity employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, reproductive health decisions, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, genetic information, political views or activity, or other applicable legally protected characteristics. You may view our Equal Employment Opportunity notice here.Meta is committed to providing reasonable support (called accommodations) in our recruiting processes for candidates with disabilities, long term conditions, mental health conditions or sincerely held religious beliefs, or who are neurodivergent or require pregnancy-related support. If you need assistance or an accommodation due to a disability, fill out the  Accommodations request form .© 2025 MetaCommunity StandardsData PolicyTermsCookie Policy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titles = soup.find_all(\"h2\") + soup.find_all(\"h3\")\n",
        "\n",
        "for t in titles:\n",
        "    print(\"Title found:\", t.text.strip())\n"
      ],
      "metadata": {
        "id": "GS6Qx4E9_vVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Set up headless Chrome\n",
        "options = Options()\n",
        "options.add_argument(\"--headless\")\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "options.add_argument(\"--disable-dev-shm-usage\")\n",
        "options.add_argument(\"--user-data-dir=/tmp/chrome-data\")\n",
        "\n",
        "driver = webdriver.Chrome(options=options)\n",
        "driver.get(\"https://www.metacareers.com/jobs\")\n",
        "\n",
        "# Optional scroll to load more jobs\n",
        "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "\n",
        "html = driver.page_source\n",
        "driver.quit()\n",
        "\n",
        "# Parse HTML with BeautifulSoup\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "# 🔍 Search all <div> blocks for likely job entries\n",
        "raw_blocks = soup.find_all(\"div\")\n",
        "\n",
        "job_data = []\n",
        "for block in raw_blocks:\n",
        "    text = block.get_text(separator=\" \", strip=True)\n",
        "    if \"Engineer\" in text or \"Intern\" in text or \"Data\" in text:\n",
        "        if \"locations\" in text or \"Location\" in text:\n",
        "            job_data.append(text)\n",
        "\n",
        "# Convert to structured format (basic example)\n",
        "structured_jobs = [{\"Job_Info\": entry} for entry in job_data]\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(structured_jobs)\n",
        "df.to_csv(\"meta_jobs.csv\", index=False)\n",
        "\n",
        "print(\"✅ Scraped data saved to 'meta_jobs.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYOQ5Vm8vUFo",
        "outputId": "a937113f-0915-48a0-b450-14d100755fdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Scraped data saved to 'meta_jobs.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"meta_jobs.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "gOttcYiKwX4p",
        "outputId": "62e9bfbb-108e-40bc-c14a-c870dd85dbcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_be6982df-6e1a-4743-bcfe-fa4190e4dce9\", \"meta_jobs.csv\", 1)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Chrome headless setup\n",
        "options = Options()\n",
        "options.add_argument(\"--headless\")\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "options.add_argument(\"--disable-dev-shm-usage\")\n",
        "options.add_argument(\"--user-data-dir=/tmp/chrome-data\")\n",
        "\n",
        "driver = webdriver.Chrome(options=options)\n",
        "driver.get(\"https://www.metacareers.com/jobs\")\n",
        "\n",
        "# ✅ Wait for job titles to load\n",
        "try:\n",
        "    WebDriverWait(driver, 15).until(\n",
        "        EC.presence_of_element_located((By.XPATH, \"//div[contains(text(), 'Engineer')]\"))\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(\"Timeout waiting for job listings.\")\n",
        "    driver.quit()\n",
        "\n",
        "# Grab full page source\n",
        "html = driver.page_source\n",
        "driver.quit()\n",
        "\n",
        "# Parse with BeautifulSoup\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "# Search all <div>s for job blocks\n",
        "job_blocks = soup.find_all(\"div\")\n",
        "\n",
        "job_data = []\n",
        "\n",
        "# Extract text that looks like job info\n",
        "for block in job_blocks:\n",
        "    text = block.get_text(separator=\" \", strip=True)\n",
        "    if any(role in text for role in [\"Engineer\", \"Intern\", \"Data\", \"Scientist\"]):\n",
        "        if \"location\" in text.lower() or \"+\" in text:\n",
        "            job_data.append(text)\n",
        "\n",
        "# Remove duplicates and empty items\n",
        "job_data = list(set(filter(None, job_data)))\n",
        "\n",
        "# Store in CSV\n",
        "df = pd.DataFrame({\"Job_Info\": job_data})\n",
        "df.to_csv(\"meta_jobs.csv\", index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"meta_jobs.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "mxEQsLjqxDm3",
        "outputId": "bbe538ae-6732-4deb-8b6f-30d0d395eb78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_377905c4-d253-4570-934b-8b6adf7a653d\", \"meta_jobs.csv\", 15278)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Setup headless Chrome\n",
        "options = Options()\n",
        "options.add_argument(\"--headless\")\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "options.add_argument(\"--disable-dev-shm-usage\")\n",
        "options.add_argument(\"--user-data-dir=/tmp/chrome-data\")\n",
        "\n",
        "driver = webdriver.Chrome(options=options)\n",
        "driver.get(\"https://www.metacareers.com/jobs\")\n",
        "\n",
        "# Wait until job cards are visible\n",
        "try:\n",
        "    WebDriverWait(driver, 15).until(\n",
        "        EC.presence_of_element_located((By.XPATH, \"//div[contains(text(), 'Engineer')]\"))\n",
        "    )\n",
        "except:\n",
        "    print(\"Timeout waiting for job listings.\")\n",
        "    driver.quit()\n",
        "\n",
        "html = driver.page_source\n",
        "driver.quit()\n",
        "\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "# Step 1: Find all h2 or h3 elements with job titles\n",
        "titles = soup.find_all(['h2', 'h3'])\n",
        "\n",
        "jobs = []\n",
        "\n",
        "for title_tag in titles:\n",
        "    title = title_tag.get_text(strip=True)\n",
        "\n",
        "    # Navigate up to find a container with full job info\n",
        "    parent = title_tag.find_parent('div')\n",
        "    if not parent:\n",
        "        continue\n",
        "\n",
        "    # Get the full container text to find location/teams (heuristic)\n",
        "    full_text = parent.get_text(separator=' ', strip=True)\n",
        "\n",
        "    # Try to split based on keywords\n",
        "    if \"location\" in full_text.lower() or \"+\" in full_text:\n",
        "        jobs.append({\n",
        "            \"Title\": title,\n",
        "            \"Details\": full_text\n",
        "        })\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(jobs)\n",
        "df.to_csv(\"meta_clean_jobs.csv\", index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"meta_clean_jobs.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0b_vDDaNI4hZ",
        "outputId": "b0a63da1-3ade-4759-9712-8dcbda35cb4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_10efa796-d558-4d1a-a4b2-86e0af7b4dfc\", \"meta_clean_jobs.csv\", 1)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# Chrome headless setup\n",
        "options = Options()\n",
        "options.add_argument(\"--headless\")\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "options.add_argument(\"--disable-dev-shm-usage\")\n",
        "options.add_argument(\"--user-data-dir=/tmp/chrome-data\")\n",
        "\n",
        "# Start WebDriver\n",
        "driver = webdriver.Chrome(options=options)\n",
        "driver.get(\"https://www.metacareers.com/jobs\")\n",
        "\n",
        "# Wait a bit for the page to start rendering\n",
        "time.sleep(5)\n",
        "\n",
        "# Scroll to the bottom slowly to trigger lazy loading\n",
        "for i in range(10):  # scroll multiple times\n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "    time.sleep(2)\n",
        "\n",
        "# Wait for job cards to load\n",
        "WebDriverWait(driver, 20).until(\n",
        "    EC.presence_of_element_located((By.XPATH, \"//div[contains(text(), 'Engineer')]\"))\n",
        ")\n",
        "\n",
        "# Grab the HTML after full scroll\n",
        "html = driver.page_source\n",
        "driver.quit()\n",
        "\n",
        "# Parse HTML\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "# Extract job titles\n",
        "titles = []\n",
        "for tag in soup.find_all(['h2', 'h3']):\n",
        "    text = tag.get_text(strip=True)\n",
        "    if any(kw in text for kw in ['Engineer', 'Intern', 'Data']):\n",
        "        titles.append(text)\n",
        "\n",
        "# Remove duplicates\n",
        "titles = list(set(titles))\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame({\"Job_Title\": titles})\n",
        "df.to_csv(\"meta_clean_jobs.csv\", index=False)\n",
        "\n",
        "# Download\n",
        "from google.colab import files\n",
        "files.download(\"meta_clean_jobs.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ZSsDRVTJJRLU",
        "outputId": "db8c29f8-c53f-4e48-baaf-e6247779911c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f8bcfdcd-60f3-42fa-ba72-394da4a509d4\", \"meta_clean_jobs.csv\", 10)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Meta GraphQL endpoint\n",
        "url = \"https://www.metacareers.com/graphql\"\n",
        "\n",
        "# Headers similar to browser\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"User-Agent\": \"Mozilla/5.0\"\n",
        "}\n",
        "\n",
        "# Payload based on your captured DevTools data\n",
        "payload = {\n",
        "    \"search_input\": {\n",
        "        \"q\": None,\n",
        "        \"divisions\": [],\n",
        "        \"offices\": [],\n",
        "        \"roles\": [],\n",
        "        \"leadership_levels\": [],\n",
        "        \"saved_jobs\": [],\n",
        "        \"saved_searches\": [],\n",
        "        \"sub_teams\": [],\n",
        "        \"teams\": [],\n",
        "        \"is_leadership\": False,\n",
        "        \"is_remote_only\": False,\n",
        "        \"sort_by_new\": False,\n",
        "        \"results_per_page\": None\n",
        "    }\n",
        "}\n",
        "\n",
        "# Send the POST request\n",
        "response = requests.post(url, json=payload, headers=headers)\n",
        "\n",
        "# Parse the response\n",
        "if response.status_code == 200:\n",
        "    json_data = response.json()\n",
        "    all_jobs = json_data['data']['job_search_with_featured_jobs']['all_jobs']\n",
        "\n",
        "    # Extract structured job info\n",
        "    jobs = []\n",
        "    for job in all_jobs:\n",
        "        jobs.append({\n",
        "            \"Title\": job.get(\"title\"),\n",
        "            \"Location\": job.get(\"location\"),\n",
        "            \"Teams\": \", \".join(job.get(\"teams\", [])),\n",
        "            \"Apply Link\": f\"https://www.metacareers.com/jobs/{job.get('slug')}\"\n",
        "        })\n",
        "\n",
        "    # Save to CSV\n",
        "    df = pd.DataFrame(jobs)\n",
        "    df.to_csv(\"meta_jobs_graphql.csv\", index=False)\n",
        "\n",
        "    from google.colab import files\n",
        "    files.download(\"meta_jobs_graphql.csv\")\n",
        "\n",
        "    print(f\"✅ Scraped {len(jobs)} jobs and saved to CSV!\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ Request failed. Status code:\", response.status_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbFRJzt6sKcH",
        "outputId": "1b255d20-9238-442f-e050-ef03172e3f34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Request failed. Status code: 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Correct Meta GraphQL endpoint\n",
        "url = \"https://www.metacareers.com/graphql\"\n",
        "\n",
        "# Standard browser headers\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"User-Agent\": \"Mozilla/5.0\"\n",
        "}\n",
        "\n",
        "# Correct GraphQL payload (query + variables)\n",
        "payload = {\n",
        "    \"query\": \"\"\"\n",
        "    query JobSearchWithFeaturedJobsQuery(\n",
        "      $searchInput: JobSearchInput!\n",
        "    ) {\n",
        "      job_search_with_featured_jobs(search_input: $searchInput) {\n",
        "        all_jobs {\n",
        "          id\n",
        "          title\n",
        "          slug\n",
        "          location\n",
        "          teams\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    \"\"\",\n",
        "    \"variables\": {\n",
        "        \"searchInput\": {\n",
        "            \"q\": None,\n",
        "            \"divisions\": [],\n",
        "            \"offices\": [],\n",
        "            \"roles\": [],\n",
        "            \"leadership_levels\": [],\n",
        "            \"saved_jobs\": [],\n",
        "            \"saved_searches\": [],\n",
        "            \"sub_teams\": [],\n",
        "            \"teams\": [],\n",
        "            \"is_leadership\": False,\n",
        "            \"is_remote_only\": False,\n",
        "            \"sort_by_new\": False,\n",
        "            \"results_per_page\": 50\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Send request\n",
        "response = requests.post(url, json=payload, headers=headers)\n",
        "\n",
        "# Parse response\n",
        "if response.status_code == 200:\n",
        "    data = response.json()\n",
        "    job_list = data['data']['job_search_with_featured_jobs']['all_jobs']\n",
        "\n",
        "    # Build structured list\n",
        "    jobs = []\n",
        "    for job in job_list:\n",
        "        jobs.append({\n",
        "            \"Title\": job.get(\"title\"),\n",
        "            \"Location\": job.get(\"location\"),\n",
        "            \"Teams\": \", \".join(job.get(\"teams\", [])),\n",
        "            \"Apply Link\": f\"https://www.metacareers.com/jobs/{job.get('slug')}\"\n",
        "        })\n",
        "\n",
        "    # Save to CSV\n",
        "    df = pd.DataFrame(jobs)\n",
        "    df.to_csv(\"meta_jobs_fixed.csv\", index=False)\n",
        "\n",
        "    from google.colab import files\n",
        "    files.download(\"meta_jobs_fixed.csv\")\n",
        "\n",
        "    print(f\"✅ Scraped {len(jobs)} jobs and saved to meta_jobs_fixed.csv!\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ Request failed with status code:\", response.status_code)\n",
        "    print(\"Message:\", response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO2DKV34scUs",
        "outputId": "f4ba03f8-957e-4754-bb84-48d2d820c719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Request failed with status code: 400\n",
            "Message: <!DOCTYPE html><html lang=\"en\" id=\"facebook\"><head><title>Error</title><meta charset=\"utf-8\" /><meta http-equiv=\"Cache-Control\" content=\"no-cache\" /><meta name=\"robots\" content=\"noindex,nofollow\" /><style nonce=\"jQEeJ7Q1\">html, body { color: #333; font-family: 'Lucida Grande', 'Tahoma', 'Verdana', 'Arial', sans-serif; margin: 0; padding: 0; text-align: center;}\n",
            "#header { height: 30px; padding-bottom: 10px; padding-top: 10px; text-align: center;}\n",
            "#icon { width: 30px;}\n",
            ".core { margin: auto; padding: 1em 0; text-align: left; width: 904px;}\n",
            "h1 { font-size: 18px;}\n",
            "p { font-size: 13px;}\n",
            ".footer { border-top: 1px solid #ddd; color: #777; float: left; font-size: 11px; padding: 5px 8px 6px 0; width: 904px;}</style></head><body><div id=\"header\"><a href=\"//www.facebook.com/\"><img id=\"icon\" src=\"//static.facebook.com/images/logos/facebook_2x.png\" /></a></div><div class=\"core\"><h1>Sorry, something went wrong.</h1><p>We&#039;re working on getting this fixed as soon as we can.</p><p><a id=\"back\" href=\"//www.facebook.com/\">Go back</a></p><div class=\"footer\"> Meta &#169; 2025 &#183; <a href=\"//www.facebook.com/help/?ref=href052\">Help</a></div></div><script nonce=\"jQEeJ7Q1\">\n",
            "              document.getElementById(\"back\").onclick = function() {\n",
            "                if (history.length > 1) {\n",
            "                  history.back();\n",
            "                  return false;\n",
            "                }\n",
            "              };\n",
            "            </script></body></html><!-- @codegen-command : phps GenerateErrorPages --><!-- @generated SignedSource<<a2e14b6d3c6a99777af41324d62f78e6>> -->\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Setup Chrome & Selenium in Colab\n",
        "!apt-get update > /dev/null\n",
        "!apt install chromium-chromedriver > /dev/null\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium > /dev/null\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Configure headless Chrome\n",
        "options = Options()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.add_argument('--user-data-dir=/tmp/chrome-data')\n",
        "\n",
        "driver = webdriver.Chrome(options=options)\n",
        "driver.get(\"https://www.metacareers.com/jobs\")\n",
        "\n",
        "# Wait for page and JavaScript to load\n",
        "time.sleep(10)\n",
        "\n",
        "# Step 2: Extract the full page source\n",
        "html = driver.page_source\n",
        "driver.quit()\n",
        "\n",
        "# Step 3: Extract window.__APOLLO_STATE__ using regex\n",
        "import re\n",
        "apollo_match = re.search(r'window\\.__APOLLO_STATE__\\s*=\\s*({.*?});', html)\n",
        "\n",
        "if not apollo_match:\n",
        "    print(\"❌ Could not find Apollo State in page source.\")\n",
        "else:\n",
        "    apollo_json = apollo_match.group(1)\n",
        "    apollo_data = json.loads(apollo_json)\n",
        "\n",
        "    # Step 4: Extract job entries from Apollo cache\n",
        "    jobs = []\n",
        "    for key, value in apollo_data.items():\n",
        "        if key.startswith(\"Job:\") and isinstance(value, dict):\n",
        "            title = value.get(\"title\")\n",
        "            location = value.get(\"location\")\n",
        "            slug = value.get(\"slug\")\n",
        "            teams = \", \".join(value.get(\"teams\", []))\n",
        "            link = f\"https://www.metacareers.com/jobs/{slug}\" if slug else \"\"\n",
        "\n",
        "            jobs.append({\n",
        "                \"Title\": title,\n",
        "                \"Location\": location,\n",
        "                \"Teams\": teams,\n",
        "                \"Apply Link\": link\n",
        "            })\n",
        "\n",
        "    # Step 5: Save to CSV\n",
        "    df = pd.DataFrame(jobs)\n",
        "    df.to_csv(\"meta_jobs_apollo.csv\", index=False)\n",
        "\n",
        "    from google.colab import files\n",
        "    files.download(\"meta_jobs_apollo.csv\")\n",
        "\n",
        "    print(f\"✅ Extracted {len(jobs)} jobs from Apollo state and saved to CSV.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0zI1frctQ2r",
        "outputId": "fe88f1c7-be95-47bc-d12c-d3c844e5e3e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "❌ Could not find Apollo State in page source.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Selenium and Chrome\n",
        "!apt-get update > /dev/null\n",
        "!apt install chromium-chromedriver > /dev/null\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium > /dev/null\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# Headless Chrome config\n",
        "options = Options()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.add_argument('--user-data-dir=/tmp/chrome-data')\n",
        "\n",
        "driver = webdriver.Chrome(options=options)\n",
        "driver.get(\"https://www.metacareers.com/jobs\")\n",
        "\n",
        "# Scroll and wait\n",
        "for _ in range(10):\n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "    time.sleep(2)\n",
        "\n",
        "html = driver.page_source\n",
        "driver.quit()\n",
        "\n",
        "# Parse HTML\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "cards = soup.find_all(\"a\", href=True)\n",
        "\n",
        "jobs = []\n",
        "\n",
        "for card in cards:\n",
        "    href = card[\"href\"]\n",
        "    if \"/jobs/\" not in href:\n",
        "        continue\n",
        "\n",
        "    raw_text = card.get_text(separator=\"|\", strip=True)\n",
        "    parts = raw_text.split(\"|\")\n",
        "\n",
        "    if len(parts) >= 3:\n",
        "        title = parts[0]\n",
        "        location = parts[1]\n",
        "        team = parts[2]\n",
        "    else:\n",
        "        title, location, team = parts + [\"\"] * (3 - len(parts))\n",
        "\n",
        "    jobs.append({\n",
        "        \"Title\": title,\n",
        "        \"Location\": location,\n",
        "        \"Team\": team,\n",
        "        \"Apply Link\": f\"https://www.metacareers.com{href}\"\n",
        "    })\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(jobs).drop_duplicates()\n",
        "df.to_csv(\"meta_structured_jobs.csv\", index=False)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"meta_structured_jobs.csv\")\n",
        "\n",
        "print(f\"✅ Extracted {len(df)} structured job listings.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "NnSqlk1fCkWk",
        "outputId": "dab0cc13-aa23-4ef8-f913-0700eeee0df5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fe11d46a-bf8f-4f91-ae27-36b487eb35dc\", \"meta_structured_jobs.csv\", 1602)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extracted 13 structured job listings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mysql-connector-python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnxOzPaSQu6h",
        "outputId": "a45ef50e-fd00-402c-a525-e4fbc8e027d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mysql-connector-python\n",
            "  Downloading mysql_connector_python-9.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
            "Downloading mysql_connector_python-9.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (33.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.9/33.9 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mysql-connector-python\n",
            "Successfully installed mysql-connector-python-9.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import mysql.connector\n",
        "\n",
        "# Load CSV file\n",
        "df = pd.read_excel(\"/content/Meta_Scraped_data.csv.xlsx\")\n",
        "print(df.head())\n",
        "\n",
        "# Connect to MySQL\n",
        "conn = mysql.connector.connect(\n",
        "    host=\"localhost\",\n",
        "    user=\"root\",           # e.g., \"root\"\n",
        "    password=\"MangekyoTota123#\",       # e.g., \"mypassword\"\n",
        "    database=\"metajobs_scraped\"     # your schema name\n",
        ")\n",
        "\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Optional: clear existing data before import\n",
        "# cursor.execute(\"DELETE FROM meta_job\")\n",
        "\n",
        "# Insert each row into the meta_job table\n",
        "for _, row in df.iterrows():\n",
        "    sql = \"\"\"\n",
        "    INSERT INTO meta_job (title, location, apply_link)\n",
        "    VALUES (%s, %s, %s, %s)\n",
        "    \"\"\"\n",
        "    cursor.execute(sql, (\n",
        "        row['Title'],\n",
        "        row['Location'],\n",
        "        row['Apply Link']\n",
        "    ))\n",
        "\n",
        "conn.commit()\n",
        "cursor.close()\n",
        "conn.close()\n",
        "\n",
        "print(\"✅ Data from CSV inserted into MySQL successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "iq41L2_BQyuy",
        "outputId": "ff9ee68a-41f9-403d-cc4b-9bb58a1d6775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               Title  \\\n",
            "0                  Software Engineer, Infrastructure   \n",
            "1                Software Engineer, Machine Learning   \n",
            "2  Research Scientist, Systems and Infrastructure...   \n",
            "3                                NPI Program Manager   \n",
            "4                        Critical Operations Manager   \n",
            "\n",
            "                       Location  \\\n",
            "0   Sunnyvale, CA +13 locations   \n",
            "1  Northridge, CA +17 locations   \n",
            "2    Sunnyvale, CA +6 locations   \n",
            "3                   Fremont, CA   \n",
            "4                  Cheyenne, WY   \n",
            "\n",
            "                                          Apply Link  \n",
            "0  https://www.metacareers.com/jobs/1408007706638...  \n",
            "1  https://www.metacareers.com/jobs/774198984091403/  \n",
            "2  https://www.metacareers.com/jobs/912427707598266/  \n",
            "3  https://www.metacareers.com/jobs/2442368328388...  \n",
            "4  https://www.metacareers.com/jobs/740988315557037/  \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InterfaceError",
          "evalue": "2003: Can't connect to MySQL server on 'localhost:3306' (Errno 111: Connection refused)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mysql/connector/network.py\u001b[0m in \u001b[0;36mopen_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msockaddr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mInterfaceError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-4148076901>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Connect to MySQL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m conn = mysql.connector.connect(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"localhost\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"root\"\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0;31m# e.g., \"root\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mysql/connector/pooling.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mCMySQLConnection\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_pure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mCMySQLConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mMySQLConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mysql/connector/connection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;31m# Tidy-up underlying socket on failure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mysql/connector/abstracts.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         charset, collation = (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mysql/connector/connection.py\u001b[0m in \u001b[0;36m_open_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    409\u001b[0m                     \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 ) from err\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0;31m# as the connection is established, set back the read\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mysql/connector/connection.py\u001b[0m in \u001b[0;36m_open_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;31m# do initial handshake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mysql/connector/network.py\u001b[0m in \u001b[0;36mopen_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    804\u001b[0m             ) from err\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             raise InterfaceError(\n\u001b[0m\u001b[1;32m    807\u001b[0m                 \u001b[0merrno\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2003\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_port\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_strioerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInterfaceError\u001b[0m: 2003: Can't connect to MySQL server on 'localhost:3306' (Errno 111: Connection refused)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SETUP (Colab) ---\n",
        "!apt-get update > /dev/null\n",
        "!apt install chromium-chromedriver > /dev/null\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium mysql-connector-python > /dev/null\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import mysql.connector\n",
        "import time\n",
        "\n",
        "# === CONFIG: Only change these ===\n",
        "MYSQL_USER = \"your_mysql_username\"\n",
        "MYSQL_PASSWORD = \"your_mysql_password\"\n",
        "MYSQL_DB = \"metajobs_scraped\"\n",
        "MYSQL_TABLE = \"meta_job\"\n",
        "\n",
        "# --- Headless Chrome Setup ---\n",
        "options = Options()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.add_argument('--user-data-dir=/tmp/chrome-data')\n",
        "\n",
        "driver = webdriver.Chrome(options=options)\n",
        "driver.get(\"https://www.metacareers.com/jobs\")\n",
        "\n",
        "# Scroll to load jobs\n",
        "for _ in range(10):\n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "    time.sleep(2)\n",
        "\n",
        "html = driver.page_source\n",
        "driver.quit()\n",
        "\n",
        "# --- Parse HTML ---\n",
        "soup = BeautifulSoup(html, \"html.parser\")\n",
        "cards = soup.find_all(\"a\", href=True)\n",
        "\n",
        "jobs = []\n",
        "\n",
        "for card in cards:\n",
        "    href = card[\"href\"]\n",
        "    if \"/jobs/\" not in href:\n",
        "        continue\n",
        "\n",
        "    raw_text = card.get_text(separator=\"|\", strip=True)\n",
        "    parts = raw_text.split(\"|\")\n",
        "    title, location, team = parts + [\"\"] * (3 - len(parts))  # Fill missing\n",
        "\n",
        "    jobs.append((title, location, team, f\"https://www.metacareers.com{href}\"))\n",
        "\n",
        "# --- MySQL: Connect to Server (no DB yet) ---\n",
        "conn = mysql.connector.connect(\n",
        "    host=\"localhost\",\n",
        "    user=MYSQL_USER,\n",
        "    password=MYSQL_PASSWORD\n",
        ")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# --- Create DB and Table if not exists ---\n",
        "cursor.execute(f\"CREATE DATABASE IF NOT EXISTS {MYSQL_DB}\")\n",
        "cursor.execute(f\"USE {MYSQL_DB}\")\n",
        "\n",
        "cursor.execute(f\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS {MYSQL_TABLE} (\n",
        "        id INT AUTO_INCREMENT PRIMARY KEY,\n",
        "        title VARCHAR(255),\n",
        "        location VARCHAR(255),\n",
        "        team VARCHAR(255),\n",
        "        apply_link TEXT\n",
        "    )\n",
        "\"\"\")\n",
        "\n",
        "# --- Insert Jobs ---\n",
        "for job in jobs:\n",
        "    cursor.execute(f\"\"\"\n",
        "        INSERT INTO {MYSQL_TABLE} (title, location, team, apply_link)\n",
        "        VALUES (%s, %s, %s, %s)\n",
        "    \"\"\", job)\n",
        "\n",
        "conn.commit()\n",
        "cursor.close()\n",
        "conn.close()\n",
        "\n",
        "print(f\"✅ Successfully inserted {len(jobs)} jobs into {MYSQL_DB}.{MYSQL_TABLE}\")\n"
      ],
      "metadata": {
        "id": "QE2v6GC3n7ve"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}